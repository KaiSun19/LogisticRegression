{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Workshop",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvaI1d0lgoLm"
      },
      "source": [
        "# **Scikit- Learn : Logistic Regression** \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oP3dXJ6jC3k"
      },
      "source": [
        "## An overview of Logistic Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iNCZkKKkAOp"
      },
      "source": [
        "Whereas linear regression deals with continous dependant variables such as House Prices, logistic regression deals with calculating the probability of an  outcome that is categorical by nature, e.g. whether someone has diabetes or not . This makes it a method of solving classification problems. \n",
        "\n",
        "A logistic regression function *P(x)*  is found, such that the predicted outcomes *p(Xi)*, is as close as possible to the actual outcomes *Yi* for each observation. \n",
        "\n",
        "First, the dependant variable is expressed as a  linear function  or *logit*: \n",
        "\n",
        "\n",
        "$ y = β0 + β1X1 + β2X2 +... + βnXn $\n",
        "\n",
        "\n",
        "where *Xn* are the dependant variables and *βn* are the coefficients or *estimators* that will be calculated. \n",
        "\n",
        "The linear function is then applied to the general sigmoid equation :\n",
        "\n",
        "\n",
        "$ p = 1 / (1 + e$^-y$) $\n",
        "\n",
        "\n",
        "To form $  p = 1 / (1 + e$^- β0 + β1X1 + β2X2 +... + βnXn$) $\n",
        "\n",
        "Whereas linear regression is solved using Ordinary Least Squares, a logistic regression is solved using the Maximum Likelihood Estimation Approach \n",
        "\n",
        "The sigmoid function can be used to plot a sigmoid curve like so :\n",
        "\n",
        "![Sigmoid Curve](https://miro.medium.com/max/1280/1*OUOB_YF41M-O4GgZH_F2rw.png)\n",
        "\n",
        "\n",
        "where all outputs lie in between 1 meaning \"Yes\" or 0 meaning \"No\", with 0.5 being the default threshold value. The reason for choosing a sigmoid function being that most values would be close to 1 or 0 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_OeDRlwHyJ"
      },
      "source": [
        "##An example of a single variate binary classification problem ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPoxrHrTwU_O"
      },
      "source": [
        " 1. Import all necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5oLvqqeCDmU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score \n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfyYUVriwlil"
      },
      "source": [
        "2. Generate data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N69qJH04CIWZ"
      },
      "source": [
        "x = np.arange(10).reshape(-1, 1)\n",
        "y = np.array([0,0,0,0,1,1,1,1,1,1])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvpNcXaHx5m5"
      },
      "source": [
        "The .reshape() function is used as the model requires columns of data. -1 is used to get the needed number of rows and 1 is used to get 1 column of data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrjJ4_BZyNUy"
      },
      "source": [
        "3. Create and train the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdmpsZk_CLqO"
      },
      "source": [
        "model = LogisticRegression(solver = \"liblinear\",random_state = 0 )\n",
        "model.fit(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRd2MJtIyhX4"
      },
      "source": [
        "Note we use the default solver and a random state of 0 to ensure that the same sequence of random numbers is generated during the training process.\n",
        " .fit() used to calculate the coefficients ,*βn*, in the linear function. \n",
        "The model instance is returned "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdc3-TcVzpW4"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
        "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
        "                   warm_start=False)\n",
        "```\n",
        "\n",
        "For information about the parameters used for the model please go to :\n",
        " \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In8FDCtCzty_"
      },
      "source": [
        "The intercept and coefficient of the linear function can also be found "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlCxmiwCQPI"
      },
      "source": [
        "intercept = model.intercept_ \n",
        "coefficient = model.coef_"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG5FVKk00YCP"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "intercept = array([-1.04608067])\n",
        "coefficient = array([[0.51491375]])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXEqMuwO03VX"
      },
      "source": [
        "4. Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPbW5LxGCfFN"
      },
      "source": [
        "We can check the matrix of probabilities using "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVTDpnhOCZOz"
      },
      "source": [
        "model.predict.proba(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1IK5bdo05gn"
      },
      "source": [
        "which gives the output:\n",
        "\n",
        "\n",
        "```\n",
        "array([[0.74002157, 0.25997843],\n",
        "       [0.62975524, 0.37024476],\n",
        "       [0.5040632 , 0.4959368 ],\n",
        "       [0.37785549, 0.62214451],\n",
        "       [0.26628093, 0.73371907],\n",
        "       [0.17821501, 0.82178499],\n",
        "       [0.11472079, 0.88527921],\n",
        "       [0.07186982, 0.92813018],\n",
        "       [0.04422513, 0.95577487],\n",
        "       [0.02690569, 0.97309431]])\n",
        "```\n",
        "Each row is an observation. The first column is the probability of the output being 0 and the second column is the probability that the output is 1. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abAsgvQiC50g"
      },
      "source": [
        "Using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtahResqCqUL"
      },
      "source": [
        "model.predict(x) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9PFUSvX1xjv"
      },
      "source": [
        "Using \n",
        "\n",
        "gives the actual prediction results \n",
        "\n",
        "```\n",
        "[0 0 0 1 1 1 1 1 1 1]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUOIWZZYDCkJ"
      },
      "source": [
        "results = model.predict(x)\n",
        "prediction_matrix = model.predict_proba(x)\n",
        "p_x = prediction_matrix[:,1]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.grid(True)\n",
        "x_val = range(10)\n",
        "y = [ coefficient * x  +  intercept for x in x_val  ] \n",
        "plt.title(\"Visual illustration of the results \")\n",
        "plt.plot(x_val,p_x,label=\"Sigmoid Function\")\n",
        "plt.plot(x_val,results,\"g.\",markersize=20, label=\"Predictions\")\n",
        "plt.plot(3,1,\"rx\",markersize=20,label=\"Incorrect Prediction\")\n",
        "plt.plot(3,0,\"b.\",markersize=20, label = \"Correct Prediction\")\n",
        "plt.plot(x_val, np.squeeze(y), label=\"Linear Function\")\n",
        "plt.axhline(y=0.5, color='y', linestyle='--')\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGC7eMmBn97u"
      },
      "source": [
        "![Example Illustration](https://raw.githubusercontent.com/KaiSun19/LogisticRegression/figures/example_illustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJR15unMjgn"
      },
      "source": [
        "The above figure shows the results of the binary classification problem. As you can see there was one false positive result shown by the red cross, which should have been a negative result shown by the blue dot. As well, slightly above the x value of 2 is where the linear function gives an output of 0, and thus where the sigmoid function crosses above the 0.5 threshold as shown by the yellow line, separating the outcomes as 0s or 1s. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSLQOroRRWF2"
      },
      "source": [
        "##Real life use of binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVCJzm1ZM9o-"
      },
      "source": [
        "**Problem**: \n",
        "\n",
        "The dataset given contains medical information e.g. number of previous pregnancies, insulin level, blood glucose level, and BMI. As well, each observation has the dependant variable \"Outcome\" that is either \"1\" meaning they have diabetes and \"0\" meaning they do not. \n",
        "\n",
        "The task is to use logistic regression to accurately predict if a patient has diabetes given their medical information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iMcdfQ17M72"
      },
      "source": [
        "**Step 1 :** Load the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5973ortlE_eG"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/KaiSun19/LogisticRegression/data/diabetes.csv\"\n",
        "dataset = pd.read_csv(url)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfeOG2pG7jZ6"
      },
      "source": [
        "**Step 2**: Analyze the dataset to find features and target variables \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3QCASzGFC5D"
      },
      "source": [
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "print(dataset.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jve8pkpy8WV8"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  DiabetesPedigreeFunction  Age  Outcome\n",
        "0            6      148             72             35        0  33.6                     0.627   50        1\n",
        "1            1       85             66             29        0  26.6                     0.351   31        0\n",
        "2            8      183             64              0        0  23.3                     0.672   32        1\n",
        "3            1       89             66             23       94  28.1                     0.167   21        0\n",
        "4            0      137             40             35      168  43.1                     2.288   33        1\n",
        "5            5      116             74              0        0  25.6                     0.201   30        0\n",
        "6            3       78             50             32       88  31.0                     0.248   26        1\n",
        "7           10      115              0              0        0  35.3                     0.134   29        0\n",
        "8            2      197             70             45      543  30.5                     0.158   53        1\n",
        "9            8      125             96              0        0   0.0                     0.232   54        1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz26MKSM8e49"
      },
      "source": [
        "As we can see, the feature variables are numerical and the \"Outcome\" column is a target variable of binary nature .  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xinELhZbFF6p"
      },
      "source": [
        "features = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
        "target = \"Outcome\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj8g4neT88Lc"
      },
      "source": [
        "We then allocate the variable \"X\" to consist of our feature values and \"y\" to consist of the corresponding target values  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgeOxUbUFQL0"
      },
      "source": [
        "X = dataset[features]\n",
        "y = dataset[target]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974o7e1t-IAi"
      },
      "source": [
        "Step 3: Training the Logistic Regression Model  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abdJjwS_-eNB"
      },
      "source": [
        "The dataset is split into a training set to fit the model, and a test set to see how well the model works on unseen data. The test_size parameter is set to 0.25 meaning 0.25 of the dataset will be the test set, and the random_state is set to 0 to ensure the same sequence of random numbers are generated everytime the dataset is split, ensuring reproducibility. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spFqa4zgFS-e"
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPxYeNZp_Nd1"
      },
      "source": [
        "The model variable is used to create an instance of the Logistic Regression classifier. \n",
        "\n",
        "We use \"liblinear\" which is the algorithm used, C = 0.05  , the regularization strength set as 0.05 to prevent overfitting, and a random_state of 0 to generate random slices of data during the training process. \n",
        "\n",
        "We then fit the model to the training data using .fit() function and predict outputs given features in the test set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZTa5KL4FbaE"
      },
      "source": [
        "model = LogisticRegression(solver='liblinear', C=0.05, multi_class='ovr',\n",
        "                           random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpPTGK0SCYsn"
      },
      "source": [
        "Step 4: Scoring the data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oecQ3XHuP6hb"
      },
      "source": [
        "A confusion matrix shows predictions as true negatives ( correct 0s ), false positives ( incorrect 1s) , true positives ( correct 1s ), and false negatives ( incorrect 0s ). As the problem involves binary classification, the matrix would be a  2 *2 matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc6wnvz1Ff3D"
      },
      "source": [
        "cm = confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkafch1sRkCI"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "[[115  15]\n",
        " [ 32  30]]\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swbfTbEsSgJ_"
      },
      "source": [
        "A visualisation of the confusion matrix can be shown to give clarification using a heatmap. This is done using the sns.heatmap() function from the \"seaborn\" library. The matrix \"cm\" is converted to a dataframe and then passed as an argument.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQl9T9TaFi5p"
      },
      "source": [
        "class_names=[0,1] \n",
        "plt.xticks(class_names)\n",
        "plt.yticks(class_names)\n",
        "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPETrc-j1VUw"
      },
      "source": [
        "![Confusion Matrix](https://raw.githubusercontent.com/KaiSun19/LogisticRegression/figures/workshop_cm.png)\n",
        "\n",
        "As we can see, similar values are represented by similar colours. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKfwXOUvTFdd"
      },
      "source": [
        "\n",
        "*   The .accuracy_score() function shows how many correct predictions were made out of total predictions. \n",
        "*   The .precision_score() function shows how many times a model will make a correct prediction\n",
        "* The .recall_score() function shows the probability the model can identify a correct positive outcome \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDNNdzRlFn8X"
      },
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJS015PIUskF"
      },
      "source": [
        "\n",
        "```\n",
        "Accuracy: 0.7552083333333334\n",
        "Precision: 0.6666666666666666\n",
        "Recall: 0.4838709677419355\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hud__7dUVH0y"
      },
      "source": [
        "**Step 5: Storing the classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkAKtAjwVtlR"
      },
      "source": [
        "It is often useful to store the model into a file via pickling so that is can be used for later purposes without having to train the model again to save time. First, a new file is created via the open(\"filename\", \"wb\") function and the model is dumped into the new file. This is saved as the variable \"f\". \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "f = open(\"logisticregression.pickle\", \"wb\").pickle.dump(model,f)\n",
        "f.close()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLC9ksGHWVZC"
      },
      "source": [
        "Next,  we use open(\"filename\", \"rb\") to open the file then save the read file as a variable, \"pickle_in\". We can then load in the classifier, and use it for predictions again as if it was trained "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o06gWvkFvq8",
        "outputId": "1934cbef-74f1-4b4b-ca4d-7f1898638ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f = open(\"logisticregression.pickle\", \"wb\")\n",
        "pickle.dump(model,f)\n",
        "f.close()\n",
        "pickle_in = open(\"logisticregression.pickle\", \"rb\")\n",
        "classifier = pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\" + \" \" + str(accuracy))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7552083333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SohPZZB8jIxz"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Accuracy: 0.7552083333333334\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ_tc6IFm4Ed"
      },
      "source": [
        "#**A note about Multi-Collinearity**\n",
        "\n",
        "Multi-Collinearity is the degree to which the independant variables, pregnancies, glucose levels, etc are correlated to each other. A model's accuracy can be compromised if there is a high degree of multi-collinearity. This is because if one variable affects another, it becomes harder to determine the degree one independant variable affects the dependant variable, making coefficients change significantly when different independant variable values are used. \n",
        "\n",
        "We can test if this problem is present in the data using \n",
        "\n",
        "\n",
        "```\n",
        "X.corr() \n",
        "``` \n",
        "as \"X\" variable gives our independant values \n",
        "\n",
        "![Correlation Matrix](https://raw.githubusercontent.com/KaiSun19/LogisticRegression/figures/correlation_matrix_workshop.png) \n",
        "\n",
        "As we can see, there is not a high degree of multi-colinearity with only Pregnancies vs Age and Insulin vs Skin Thickness being somewhat correlated. \n",
        "\n",
        "Nevertheless, it is always worthwhile to consider this as a potential issue when evaluating the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYEaRq5EuiAY"
      },
      "source": [
        "##**References** \n",
        "diabetes.csv : https://www.kaggle.com/uciml/pima-indians-diabetes-database"
      ]
    }
  ]
}